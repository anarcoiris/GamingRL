# GamingRL - Proyecto Acelerado de Reinforcement Learning

![Status: W0-W3 Complete](https://img.shields.io/badge/Workflow-0--3_Complete-green)
![Python](https://img.shields.io/badge/Python-3.10+-blue)
![PyTorch](https://img.shields.io/badge/PyTorch-2.0+-orange)
![License](https://img.shields.io/badge/License-MIT-lightgrey)

Plataforma integral de investigaciÃ³n para agentes de Reinforcement Learning en juegos de mesa, progresando desde Damas (Checkers) hacia entornos de alta complejidad. El proyecto prioriza la arquitectura modular, la reproducibilidad y la instrumentaciÃ³n profunda.

## ğŸ— Arquitectura del Sistema

El sistema se compone de tres pilares desacoplados que permiten iteraciÃ³n rÃ¡pida y aislamiento de responsabilidades.

```
GamingRL/
â”œâ”€â”€ env/                 # Core LÃ³gico
â”‚   â”œâ”€â”€ checkers_env.py  # Gym compliant wrapper
â”‚   â””â”€â”€ rules.py         # Motor de reglas puro y determinista
â”œâ”€â”€ agent/               # Cerebro
â”‚   â”œâ”€â”€ dqn.py           # ImplementaciÃ³n DQN optimizada
â”‚   â””â”€â”€ network.py       # Arquitecturas CNN espaciales
â”œâ”€â”€ viz/                 # Ojos e InstrumentaciÃ³n
â”‚   â”œâ”€â”€ tb_logger.py     # IntegraciÃ³n profunda con TensorBoard
â”‚   â”œâ”€â”€ hooks.py         # AnÃ¡lisis de activaciones/gradientes
â”‚   â””â”€â”€ board_renderer.py # VisualizaciÃ³n Rich/ASCII
```

### CaracterÃ­sticas Clave
*   **Entorno Vectorizado**: RepresentaciÃ³n de estado tensor `(4, 8, 8)` ideal para CNNs.
*   **Agente DQN Robusto**: Buffer de repeticiÃ³n circular, target networks, y clipping de gradientes.
*   **Observabilidad Total**: Pipeline de logging que permite inspeccionar la "caja negra" de la red neuronal capa por capa.

## ğŸš€ Quick Start

### 1. InstalaciÃ³n
```bash
pip install -r requirements.txt
```

### 2. Entrenar Agente (Training Loop)
Entrena un agente DQN desde cero. Los checkpoints se guardan automÃ¡ticamente.
```bash
python training/train_dqn.py --num_steps 10000 --output_dir checkpoints/demo
```

### 3. Visualizar Progreso
Monitorea mÃ©tricas de pÃ©rdida, recompensa y evoluciÃ³n de pesos en tiempo real.
```bash
tensorboard --logdir logs/
```

### 4. SimulaciÃ³n RÃ¡pida
Ejecuta partidas de prueba con agentes aleatorios para validar el entorno.
```bash
python examples/play_random.py
```

## ğŸ—º Estado del Proyecto

El desarrollo sigue una metodologÃ­a estricta de Workflows secuenciales.

| Workflow | Estado | Entregables Clave |
|----------|--------|-------------------|
| **W0: DefiniciÃ³n** | âœ… Completo | `DESIGN.md`, Specs, Config JSON |
| **W1: Entorno** | âœ… Completo | Gym Env, Motor de Reglas, 21+ Tests |
| **W2: DQN BÃ¡sico** | âœ… Completo | Agente Funcional, Training Loop, Checkpoints |
| **W3: VisualizaciÃ³n** | âœ… Completo | TensorBoard, Hooks de ActivaciÃ³n, Rich Renderer |
| **W4: GUI** | ğŸš§ Pendiente | Interfaz Interactiva Web/PyGame |
| **W5: Experimentos** | ğŸ“… Futuro | Benchmarking masivo |

## ğŸ“š DocumentaciÃ³n TÃ©cnica

Para profundizar en Ã¡reas especÃ­ficas:

*   **[DESIGN.md](DESIGN.md)**: Racional detrÃ¡s de las decisiones arquitectÃ³nicas (e.g., por quÃ© CNN vs MLP).
*   **[env/README.md](env/README.md)**: Detalles sobre el tensor de estado y reglas de captura forzada.
*   **[agent/README.md](agent/README.md)**: HiperparÃ¡metros del DQN y arquitectura de red.
*   **[viz/README.md](viz/README.md)**: GuÃ­a para usar hooks de introspecciÃ³n y logging avanzado.
*   **[STANDARDS.md](STANDARDS.md)**: GuÃ­a de estilo de cÃ³digo y convenciones de testing.

## ğŸ¤ ContribuciÃ³n y Desarrollo

El proyecto impone estÃ¡ndares de calidad estrictos:
1.  **Tests Obligatorios**: Todo cÃ³digo nuevo debe incluir tests en `tests/`.
2.  **Linting**: El cÃ³digo debe cumplir con `black` y `ruff`.
3.  **Workflows AtÃ³micos**: No avanzar de fase sin completar los criterios de aceptaciÃ³n previos.

Ver **[RULES.md](RULES.md)** para el protocolo completo de desarrollo.

---
*GamingRL Research Engine - 2025*
